{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Caso de estudio Clasificador Spamless Alumnos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metamatematico/hello-world/blob/master/Caso_de_estudio_Clasificador_Spamless_Alumnos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vz0SIne0V9o"
      },
      "source": [
        "## Paso01 - Lectura del conjunto de información"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6GWHeWBhn2I"
      },
      "source": [
        "# Importar las librerías que nos ayudarán a obtener la información, organizarla y procesarla\n",
        "import _____ as pd\n",
        "# Estas librerías permiten filtrar datos que imortamos desde Drive\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "\n",
        "# Indispensable para obtener documentos desde colab (Dentro de nuestra propia sesión)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85AE37iU0SaA"
      },
      "source": [
        "# Lectura del archivo que contiene el corpus con la información clasificada\n",
        "# y preprocesada (Ham VS Spam)\n",
        "spreadsheet = gc.open(\"Spamless_DataSet\") \n",
        "sheet =  spreadsheet.get_worksheet(0)\n",
        "df2 = _____.DataFrame(sheet.get_all_records())\n",
        "df2 = df2.sample(frac=1)\n",
        "df2.head(_____) \n",
        "\n",
        "# Referencia para la lectura de archivos en: https://codeday.me/es/qa/20190412/477724.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTATQzbm6Xfj"
      },
      "source": [
        "## Paso02 - Verificación y validación de la información"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iZYPh3J6dQm"
      },
      "source": [
        "# Para detectar si tenemos filas vacías\n",
        "print(\"Tamaño del data frame: \" + str(_____(_____)))\n",
        "print(\"Cantidad de filas vacías:\")\n",
        "print(df2._____().sum())\n",
        "\n",
        "# Eliminamos las filas vacías y vemos el tamaño del nuevo df\n",
        "df2 = df2._____()\n",
        "df2 = df2[df2['_____'] != \"\"]\n",
        "print(\"\\nTamaño del data frame sin vacíos: \" + str(len(df2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQeHqVCJ6kts"
      },
      "source": [
        "# Verificar cuales son las clasificaciones que tenemos en nuestro DataSet\n",
        "print(\"Clases que tenemos en el DataSet:\")\n",
        "print(df2['clase']._____())\n",
        "\n",
        "# Revisamos la cantidad de ejemplos que tenemos por cada clase\n",
        "print(\"\\nCantidad de ejemplos que tenemos por clase:\")\n",
        "print(df2['_____'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1-EZb4A6ooT"
      },
      "source": [
        "# Ploteamos la cantidad de ejemplos que tenemos de cada clase\n",
        "# para ver si éstas se encuentran balanceadas o no\n",
        "import _____._____ as plt\n",
        "\n",
        "Totales = df2['_____'].value_counts()\n",
        "plt.bar(['_____', '_____'], Totales)\n",
        "plt.xticks(rotation = 45)\n",
        "plt.title('Cantidad de ejemplos de cada clase') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J43pST82R0qS"
      },
      "source": [
        "## Paso04 - Buscar relaciones en la información"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWKWo37IRzbY"
      },
      "source": [
        "# Para analizar la información, ploteamos un histograma\n",
        "# que nos indica la frecuencia de caracteres tanto en \n",
        "# Ham como en Spam para revisar su comportamiento\n",
        "\n",
        "Spams = []\n",
        "NoSpams = []\n",
        "for clase, mensaje in zip(df2['clase'], df2['mensaje']):\n",
        "    if clase == 'Spam':\n",
        "        Spams.append(len(_____))\n",
        "    else:\n",
        "        NoSpams.append(len(_____))\n",
        "\n",
        "\n",
        "plt.hist(_____, bins=15)\n",
        "plt.hist(_____, bins=15)\n",
        "plt.legend(['Spam', 'No Spam'])\n",
        "plt.xlabel('Cantidad de Caracteres en los ejemplos') \n",
        "plt.ylabel('Total de ejemplos') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lBG2u72SF1b"
      },
      "source": [
        "# Importamos el núcleo de trabajo para hacer la tokenización\n",
        "import _____\n",
        "nlp_es = spacy.load('_____')\n",
        "\n",
        "# Gráfica de los tokens más utilizados para Spam\n",
        "Palabras_Spam = {}\n",
        "\n",
        "# Creamos el diccionario con los tokens y su frecuencia\n",
        "for clase, mensaje in zip(df2['clase'], df2['mensaje']):\n",
        "    if clase == 'Spam':\n",
        "        for token in nlp_es(_____):\n",
        "            if(Palabras_Spam.get(token._____) == None):\n",
        "                Palabras_Spam.setdefault(token._____, 1)\n",
        "            else:\n",
        "                Palabras_Spam[token._____] += 1\n",
        "\n",
        "# Creamos un nuevo diccionario que no incluya los tokens menos frecuentes\n",
        "Palabras_Spam2 = {}\n",
        "for clave, valor in zip(Palabras_Spam._____(), Palabras_Spam._____()):\n",
        "    if(valor >= 5):\n",
        "        Palabras_Spam2.setdefault(_____, _____)\n",
        "\n",
        "# Ploteamos los elementos en el diccionario\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(Palabras_Spam2.keys(), Palabras_Spam2.values())\n",
        "plt.title('Tokens más importantes para Spam') \n",
        "plt.xticks(rotation=80)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlghBujSSJKN"
      },
      "source": [
        "# Gráfica de los tokens más utilizados para No Spam\n",
        "Palabras_NoSpam = {}\n",
        "\n",
        "# Creamos el diccionario con los tokens y su frecuencia\n",
        "for clase, mensaje in zip(df2['clase'], df2['mensaje']):\n",
        "    if clase == 'NoSpam':\n",
        "        for token in nlp_es(_____):\n",
        "            if(Palabras_NoSpam.get(token._____) == None):\n",
        "                Palabras_NoSpam.setdefault(token._____, 1)\n",
        "            else:\n",
        "                Palabras_NoSpam[token._____] += 1\n",
        "\n",
        "# Creamos un nuevo diccionario que no incluya los tokens menos frecuentes\n",
        "Palabras_NoSpam2 = {}\n",
        "for clave, valor in zip(Palabras_NoSpam._____(), Palabras_NoSpam._____()):\n",
        "    if(valor >= 5):\n",
        "        Palabras_NoSpam2.setdefault(_____, _____)\n",
        "\n",
        "# Ploteamos los elementos en el diccionario\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(Palabras_NoSpam2.keys(), Palabras_NoSpam2.values())\n",
        "plt.title('Tokens más importantes para NoSpam') \n",
        "plt.xticks(rotation=80)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXGr8cC3kzfo"
      },
      "source": [
        "## Paso05 - Construcción del modelo de NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBGORhTBk4Me"
      },
      "source": [
        "# Separar la información (Dataset) en conjuntos de entrenamiento y validación\n",
        "\n",
        "# Importar la librería de Sklearn para hacer el split de la información\n",
        "from _____._____ import train_test_split\n",
        "\n",
        "# Definimos nuestra información (Nombre de la clase (Y) y valor (X))\n",
        "X = df2['mensaje']\n",
        "Y = df2['clase']\n",
        "\n",
        "# Segmentamos la información en conjuntos de entrenamiento y de validación (80 / 20)\n",
        "_____, _____, _____, _____ = train_test_split(X, Y, test_size = _____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrGZBk_CDwGA"
      },
      "source": [
        "# Se importa la librería para extracción de características y vectorización \n",
        "# de los ejemplos usando la función CountVectorizer()\n",
        "# (Permite conocer información de los datos de entrenamiento)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "#  Con esta línea se construye un diccionario (Vocabulario de palabras) y se \n",
        "# cuenta el número de palabras que hay para cada elemento del diccionario\n",
        "X_train_data = count_vect.fit_transform(X_train)\n",
        "\n",
        "# (Shape) El primer número representa los mensajes, y el segundo el vocabulario usado\n",
        "X_train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKaohP8QD0MG"
      },
      "source": [
        "# Se crea un Pipeline para asignar la secuencia de procesos\n",
        "from sklearn._____ import Pipeline\n",
        "from sklearn._____._____ import TfidfVectorizer\n",
        "from sklearn._____ import LinearSVC\n",
        "\n",
        "# En el Pipeline se mete en un arreglo la secuencia de pasos que se desea\n",
        "# seguir, o elementos que se desea enviar, en este caso 1) Vector de Tf-idf\n",
        "# 2) El modelo LinearSVC \n",
        "# En esta línea se hace la vectorización y se ejecuta el clasificador en un solo paso\n",
        "clasificador_Texto = _____([('tfidf', _____()),('clf', _____())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2QpOF_fD3kz"
      },
      "source": [
        "# Visualizamos los parámetros de nuestro modelo\n",
        "print(clasificador_Texto.get_params().keys())\n",
        "\n",
        "# Podemos cambiar algunos de los parámetros como por ejempplo, uso de minúsculas\n",
        "# filtrado de stopwords y máximo de características a usar\n",
        "clasificador_Texto.set_params(tfidf__lowercase=_____, tfidf__stop_words=[_____], tfidf__max_features=_____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGGa6z-_D6LA"
      },
      "source": [
        "clasificador_Texto.fit(_____, _____)\n",
        "Predicciones = clasificador_Texto.predict(_____)\n",
        "print(X_test)\n",
        "print(Predicciones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJYTHFfQD87M"
      },
      "source": [
        "# Escribe aquí frases con las que te gustaría comprobar el funcionamiento del modelo\n",
        "clasificador_Texto.predict([\"Frase 1\", \n",
        "                            \"Frase 2\",\n",
        "                            \"Frase 3\",\n",
        "                            \"Frase 4\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bGT9hSuIH6J"
      },
      "source": [
        "## Matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm426fdEILYI"
      },
      "source": [
        "# Matriz de confusión y Métricas de evaluación del modelo\n",
        "from sklearn._____ import _____, _____\n",
        "from sklearn import _____\n",
        "\n",
        "# Impresión de matriz de confusión\n",
        "print(\"Matriz de confusión:\")\n",
        "print(_____(_____, _____))\n",
        "\n",
        "# Impresión de procentaje de Accuracy del modelo\n",
        "print(\"\\nAccuracy del modelo: \")\n",
        "print(metrics._____(_____, _____))\n",
        "\n",
        "# Impresión de las métricas para el modelo\n",
        "print(\"\\nMétricas de evaluación:\")\n",
        "print(_____(_____, _____))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Copia de Reto_16_PCA_y_Word2Vect_para_Embeddings_Alumnos.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metamatematico/hello-world/blob/master/Copia_de_Reto_16_PCA_y_Word2Vect_para_Embeddings_Alumnos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whETFFoU3Kvw"
      },
      "source": [
        "# Reto 16: PCA y Word2Vect para Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0TFk-t3Kv3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "5487eb96-d45f-4a28-cd81-ad71eed8f80b"
      },
      "source": [
        "# Importar el modelo de word2vect de Google y las librerías necesarias\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import spacy\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nlp_es = spacy.load(\"es_core_news_sm\") "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e64aeb9245e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnlp_es\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijS7cWIx3KwF"
      },
      "source": [
        "# Usando Spacy tokenizamos las frases y las guardamos en una lista de listas\n",
        "Frases = [nlp_es(u'En la vida no existe nada que temer, solo cosas que comprender'),\n",
        "          nlp_es(u'Lo que sabemos es una gota de agua; lo que ignoramos es el océano'),\n",
        "          nlp_es(u'Vivir es enfrentar un problema tras otro. La forma en que lo encaras hace la diferencia'),\n",
        "          nlp_es(u'Hay, en verdad, dos cosas diferentes: saber y creer que se sabe. La ciencia consiste en saber; en creer que se sabe está la ignorancia'),\n",
        "          nlp_es(u'Haz las cosas lo más simple que puedas, pero no te limites a lo simple'),\n",
        "          nlp_es(u'Poco conocimiento hace que las personas se sientan orgullosas. Mucho conocimiento, que se sientan humildes'),\n",
        "          nlp_es(u'El trabajo te da significado y propósito, y la vida está vacía sin ambos'),\n",
        "          nlp_es(u'No se puede enseñar nada a un hombre; sólo se le puede ayudar a encontrar la respuesta dentro de sí mismo'),\n",
        "          nlp_es(u'Un hombre que se atreve a desperdiciar una hora no ha descubierto el valor de la vida')]\n",
        "\n",
        "Frases_Tokens = []\n",
        "for Frase in Frases:\n",
        "    Tokens = []\n",
        "    for Token in Frase:\n",
        "        # Verificar que el token no sea una stopword\n",
        "        if Token.is_stop == False:\n",
        "            Tokens.append(Token.append.lower())\n",
        "        Frases_Tokens.append(Tokens)  \n",
        "\n",
        "# Entrenamos un modelo con nuestros tokens y los Embeddings de Word2vect\n",
        "Modelo = Word2Vec(_____, _____ = 1)\n",
        "print(\"Características del modelo: \" + str(Modelo))\n",
        "\n",
        "# Obtenemos las palabras que conforman nuestro vocabulario\n",
        "# (No todas las palabras necesariamente tienen asociado un vector de embeddings)\n",
        "Palabras = list(Modelo._____.vocab)\n",
        "print(\"\\nVocabulario: \" + str(Palabras))\n",
        "\n",
        "# Podemos imprimir los valores de las características (Embeddings) asociados\n",
        "# a alguna palbra en específico (Para Word2vect existen 100 características\n",
        "# para cada token)\n",
        "print(Modelo['vida'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiKbsL2q3KwJ"
      },
      "source": [
        "# Extraemos el vocabulario del modelo y lo igresamos \n",
        "# en un PCA\n",
        "Caract = Modelo[Modelo._____.vocab]\n",
        "pca = PCA(n_components = len(_____))\n",
        "result = pca.fit_transform(_____)\n",
        "\n",
        "# Creamos un scatter plot del PCA obtenido\n",
        "# y añadimos los puntos al gráfico\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "Palabras = list(Modelo._____.vocab)\n",
        "\n",
        "# Colocamos los letreros correspondientes a cada palabra\n",
        "for i, Palabra in enumerate(Palabras):\n",
        "    _____.annotate(Palabra, xy=(result[i, 0], result[i, 1]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGDblcdb3KwN"
      },
      "source": [
        "# Es posible agarrar un modelo preentrenado y seleccionar solo la cantidad de\n",
        "# vectores de características que se quieren cargar (Mientras más se carguen \n",
        "# el proceso será más lento, pero tendrá una mayor cantidad de palabras para\n",
        "# considerar como características)\n",
        "wordvectors_file_vec = 'Word2Vect_Spanish.txt'\n",
        "cantidad = 100000\n",
        "wordvectors = KeyedVectors.load_word2vec_format(_____, limit=cantidad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKQmIEb93KwS"
      },
      "source": [
        "# Palabras cercanas con excepciones\n",
        "print(\"Palabras cercanas a sol, arena y mar, pero no a playa:\")\n",
        "print(wordvectors.most_similar_cosmul(positive=['sol', 'arena', 'mar'],negative=['playa']))\n",
        "print(\"\\nPalabras cercanas a frío y calor, pero no a clima:\")\n",
        "print(wordvectors.most_similar_cosmul(positive=['frio', 'calor'],negative=['clima']))\n",
        "print(\"\\nPalabras cercanas a piano, y guitarra, pero no a música:\")\n",
        "print(wordvectors.most_similar_cosmul(positive=['piano', 'guitarra'],negative=['música']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXimP8mu3KwX"
      },
      "source": [
        "# Implementación de la palbra que no corresponde\n",
        "Ejercicios = [['hola','adiós','buenos días','bien','buenas noches'],\n",
        "              ['febrero','septiembre','marzo','viernes','abril'],\n",
        "              ['médico','profesor','ingeniera','periodista','verdura'],\n",
        "              ['triste','feliz','amable','simpático','positivo'],\n",
        "              ['pantalones','pelo','camiseta','zapatos','bufanda'],\n",
        "              ['profesor','abuelo','tía','hermano','madre'],\n",
        "              ['autobús','naranja','plátano','pera','manzana'],\n",
        "              ['comer','beber','mirar','siempre','leer']]\n",
        "\n",
        "print(\"Palabras que no encajan en el campo semántico: \")\n",
        "for Ejercicio in Ejercicios:\n",
        "    print(_____.doesnt_match(Ejercicio))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
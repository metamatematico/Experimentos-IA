{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Creador de poesía, ajustado ",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metamatematico/Experimentos-IA/blob/master/Creador_de_poes%C3%ADa%2C_ajustado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_JA8BNWAL2"
      },
      "source": [
        "# Transferir el estilo de una obra textual a un modelo de escritura automatizada\n",
        "\n",
        "Para leer el contexto en que se generó la siguiente receta leer sobre el  [Taller de escritura computarizada](https://www.notion.so/ivanvladimir/Taller-de-escritura-computarizada-e7a7bc49b552475a92b45db6416437cd)\n",
        "\n",
        "El proceso de adaptación se lleva a cabo con los siguiente pasos\n",
        "1. Instalar las librerías de python que permiten la generación\n",
        "2. Conectarnos al google drive para guardar nuestro modelo y/o acceder la obra textual\n",
        "3. Obtener obra textual de la que se obtendrá el estilo\n",
        "4. Entrenar\n",
        "5. Generar\n",
        "\n",
        "Si ya se cuenta con un modelo entrenado, es posible brincarse el paso de entrenaiento y obtener obra (3 y 4) y pasar directamente a generar directamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXQbwUwO9FSv"
      },
      "source": [
        "## 1. Se instalan las librerias adecuadas\n",
        "\n",
        "Las siguentes librerias hechas por la empresa [Hugginface](https://huggingface.co/) permiten la adaptación (entrenamiento) y generación automatizada de textos. Las librerías son:\n",
        "1. _transformers_, librería general para manejar modelos basados en la arquitectura neruonal transformers\n",
        "2. _datasets_, librería asociada a transoformers para la manipulación de dataset/corpus de datos\n",
        "\n",
        "Al ejecutar la celda, esperar mensajes de que se descacargan archivos y se instala módulos de python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdcEOxJM9AAc",
        "outputId": "c723b6ca-7c25-4700-f7a3-2cf05c1b1186"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\r\u001b[K     |▏                               | 10kB 3.9MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 6.4MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 7.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 7.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 7.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 7.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 7.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 7.6MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 7.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204kB 7.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215kB 7.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 7.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 7.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 7.6MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 7.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 7.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276kB 7.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286kB 7.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 296kB 7.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 348kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 368kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 399kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 409kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 419kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 430kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 440kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 460kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 471kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 481kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 491kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 501kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 532kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 542kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 552kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 563kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 593kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 604kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 614kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 624kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 634kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 645kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 655kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 665kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 675kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 686kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 696kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 706kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 716kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 727kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 737kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 747kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 757kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 788kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 798kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 808kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 819kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 829kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 839kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 849kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 860kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 870kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 880kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 890kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 901kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 911kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 921kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 931kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 942kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 952kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 962kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 972kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 983kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 993kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 47.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/4f/8ef6e4df6e92fc02da620ccfac249112619e5c21273a836958160d6e96fb/datasets-1.6.1-py3-none-any.whl (220kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: huggingface-hub, xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.6.1 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLKcXbdS5_bR"
      },
      "source": [
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwawDNpbtdbW"
      },
      "source": [
        "\n",
        "## Obtener script de adaptaición (cont. 1)\n",
        "\n",
        "La transferencia de estilo, adaptación la llevaremos a cabo usando un script que forma parte de la librería de 'transformers' para lo cual es necesario descargarlo con la siguiente celda.\n",
        "\n",
        "Esperar unmensaje de que se descarga el archivo 'run_clm.py'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT-lcDTd4HNd",
        "outputId": "f6e1cb2e-6868-4594-b1e3-51f550dff97b"
      },
      "source": [
        "!wget -O run_clm.py https://raw.githubusercontent.com/huggingface/transformers/v4.4.1/examples/language-modeling/run_clm.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-26 18:03:59--  https://raw.githubusercontent.com/huggingface/transformers/v4.4.1/examples/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18700 (18K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "run_clm.py          100%[===================>]  18.26K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-04-26 18:03:59 (28.9 MB/s) - ‘run_clm.py’ saved [18700/18700]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swm8AAKopjhi"
      },
      "source": [
        "## 2. Conectar nuestro google drive\n",
        "\n",
        "La plataforma colab puede acceder a nuestro google drive, en particular esto nos sirve para guardar el modelo adaptado en google drive y volverlo a usarlo más adelante (recordar que la máquina virtual en la que se ejecua colab deja de existir depués de un tiempo de inactividad o máximo 8 horas). \n",
        "\n",
        "Para lograr la conección, ejecutar la celda, y hacer click en el link generado automáticamente; confirmar con el sistema google la conexión hasta identificar el código de acceso que se pegará en la caja de texto de la misma celda. Depupués de unos segundos aparecerá el mensaje que google drive se ha 'montado' en ua ruta \"/content/gdrive\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwfqUrYzpbMo",
        "outputId": "dcccdb1d-632c-4471-d7f8-54660851eaa5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeFEoCQcU7sj"
      },
      "source": [
        "## 3. Conseguir el texto que se utilizará para adaptar\n",
        "\n",
        "Al proceso de adaptación hay que pasarla una obra que se adaptará, para lograr esto es necesario hacerla disponible dese la plataforma de colab, se pude hacer de tres forma:\n",
        "\n",
        "1. Bajarlo desde el internet directamente al ambiente de Colab\n",
        "2. Subirlo a colab a través del menu \"Files\" ubicado en la parte izquierda de la plataform\n",
        "3. Usando la conexión con google drive\n",
        "\n",
        "En general se recomienda el formato .txt en una codificiación UTF-8; también es importante tener muy claro el nombre del archivo que contiene el texto y la ruta en la que se encuentra. Si se hace a través de los pasos 3.1 y 3.2; lo más probable es que esté accesibe desde el directotio de ejecución de colab, por lo que no haría falta fijarse en la ruta.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_fxuE3OGnGi"
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "contenido= list()\n",
        "\n",
        "def callback(pat):\n",
        "  return \"\\n\\n\"+pat.group(1).upper()+\"\\n\\n\"\n",
        "\n",
        "def callback2(pat):\n",
        "  return pat.group(1).upper()+\"\\n\\np* --\"\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Proyecto NLP/archivos de ejecucion/full_poemas_rosa_espino_manipulado.txt') as archivo:\n",
        "       for linea in archivo:\n",
        "            if not linea == '\\n':\n",
        "                linea.rstrip('\\n')\n",
        "                linea2 = ' -- '+linea+' -- \\n'\n",
        "            else:\n",
        "                linea2 = '\\n'\n",
        "            \n",
        "            l = linea2.split('\\n --')\n",
        "            \n",
        "            if l == ['\\n']:\n",
        "                l = '*p'.join(l)\n",
        "                #print(p)\n",
        "                contenido.append('*p \\n'+ l + '*p')\n",
        "                \n",
        "            elif not l == ['\\n'] :\n",
        "                l = ' -- '.join(l)\n",
        "                contenido.append((l))\n",
        "\n",
        "       contenido=\"\".join(contenido)\n",
        "       contenido=re.sub(r\"\\n\\*p \\n\\n\",\"\\n\\n\",contenido,re.DOTALL)\n",
        "       contenido=re.sub(r\"\\n\\n(.*?)\\n\\n\",callback,contenido,re.DOTALL)\n",
        "       contenido=re.sub(r\"(\\n\\*p --[A-ZÑy .]*?--\\s+?)\\n --\",callback2,contenido,re.DOTALL)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Proyecto NLP/archivos de ejecucion/full_poemas_rosa_espino_manipulado.txt', 'w',  encoding='utf-8', errors='ignore') as archivo:\n",
        "    archivo.writelines(contenido)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duZ8-PDw_8k3"
      },
      "source": [
        "### 3.1 Bajarlo desde internet\n",
        "\n",
        "Usando el comando wget obtener el archivo de una URL pública con el texto. En la celda de abajo remplazar por la URL para bajar el texto recomendado. En el ejemplo se descarga El quijote en un archivo \"el_quijote.txt\".\n",
        "\n",
        "Quidado, si la celda se ejecuta varias veces con el mismo nombre de archivo a bajar se crearán varias versiones del texto con números indicando la versión.\n",
        "\n",
        "Se espera que el archivo recuperado se baje y quede visible en el directorio de ejecución de colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_jN94DeUmkm",
        "outputId": "aeed2a74-e75f-4193-cd98-660dec18d07c"
      },
      "source": [
        "#!wget https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-12 00:44:07--  https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1060259 (1.0M) [text/plain]\n",
            "Saving to: ‘el_quijote.txt’\n",
            "\n",
            "\rel_quijote.txt        0%[                    ]       0  --.-KB/s               \rel_quijote.txt      100%[===================>]   1.01M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-03-12 00:44:08 (51.4 MB/s) - ‘el_quijote.txt’ saved [1060259/1060259]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi-tGgtIATuJ"
      },
      "source": [
        "### 3.2 Subirlo a colab\n",
        "\n",
        "Si el archivo está de forma local en la máquina utilizada para acceder a la plataforma colab, es posible subirlo a ésta. Para este proceso se usa el menu 'Files' de las izquierda siguiendo el procedimiento después de hacer click en el botón de 'Upload'. No es necesario ejecutar ninguna celda.\n",
        "\n",
        "Se espera que el archivo subido quede visible en el directorio de ejecución de colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dbDPNWBEJv"
      },
      "source": [
        "### 3.3 A través de google drive\n",
        "\n",
        "Si el archivo está en el servicio de nube de google drive, es posible acceder a ese archivo ya que tenemos conectado nuestro google drive a colab (ver paso 2). Para identificar la ruta y nombre del archivo, se puede usar el menú \"Files\" a la izquieda de la plataforma colab, localizar el archivo en el google drive, bajo el nombre oprimir el menu con tres punto \"...\" y escoger la opción \"Copiar path\". No es necesario ejecutar ninguna celda.\n",
        "\n",
        "Se espera haber identificado la ruta del archivo dentro de google drive que se usará como fuente de estilo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWhcE1gBUv7b"
      },
      "source": [
        "## 4. Entrenamiento para adpatar el modelo\n",
        "\n",
        "La siguiente celda es la que adapta un modelo pre entrenado 'datificate/gpt2-small-spanish' con el estilo del texto fuente.\n",
        "\n",
        "En este paso será necesario tener claro el nombre y la ruta (para paso 3.3) del archivo textual que se usará como fuente para el estilo (ver paso 3). Esa ruta (de ser necesaria) y nombre se sustituira en la celda de abajo para la opción '--train_file' \n",
        "\n",
        "Cuidado en el formato de la celda es muy imporante mantener el símbolo '\\' al final de la línea en la opción '--train_file' y que no haya ningún símbolo o espacio después de éste.\n",
        "\n",
        "Al ejecutar la siguiente línea puede llevarse varios minutos o hasta horas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaJDLVra2yP1",
        "outputId": "09c2eff5-dea7-4ee4-a6a7-74808721c189"
      },
      "source": [
        "!mkdir output\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path 'datificate/gpt2-small-spanish' \\\n",
        "    --train_file '/content/gdrive/MyDrive/Proyecto NLP/archivos de ejecucion/full_poemas_rosa_espino_manipulado.txt' \\\n",
        "    --do_train \\\n",
        "    --block_size 128\\\n",
        "    --num_train_epochs 30 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --save_total_limit 0 \\\n",
        "    --gradient_accumulation_steps=12 \\\n",
        "    --learning_rate=3e-5 \\\n",
        "    --seed 10 \\\n",
        "    --fp16 \\\n",
        "    --fp16 --fp16_opt_level O3 \\\n",
        "    --prediction_loss_only \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --validation_file \"/content/gdrive/MyDrive/Proyecto NLP/archivos de ejecucion/full_poemas_rosa_espino_manipulado.txt\" \\\n",
        "    --do_eval \\\n",
        "    --output_dir /output/gpt2-small-spanish-adaptado"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘output’: File exists\n",
            "2021-04-26 18:04:51.591941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/26/2021 18:04:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "04/26/2021 18:04:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/output/gpt2-small-spanish-adaptado, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=True, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=12, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=30.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr26_18-04-52_2acb726954ce, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=0, no_cuda=False, seed=10, fp16=True, fp16_opt_level=O3, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/output/gpt2-small-spanish-adaptado, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
            "04/26/2021 18:04:53 - WARNING - datasets.builder -   Using custom data configuration default-ca1501eb03341da4\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-ca1501eb03341da4/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-ca1501eb03341da4/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1426] 2021-04-26 18:04:53,709 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjg2nk4co\n",
            "Downloading: 100% 817/817 [00:00<00:00, 671kB/s]\n",
            "[INFO|file_utils.py:1430] 2021-04-26 18:04:54,061 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|file_utils.py:1433] 2021-04-26 18:04:54,061 >> creating metadata file for /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:491] 2021-04-26 18:04:54,061 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:527] 2021-04-26 18:04:54,062 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-26 18:04:54,266 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:527] 2021-04-26 18:04:54,267 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1426] 2021-04-26 18:04:54,488 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkmh6lrfq\n",
            "Downloading: 100% 850k/850k [00:00<00:00, 2.28MB/s]\n",
            "[INFO|file_utils.py:1430] 2021-04-26 18:04:55,073 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|file_utils.py:1433] 2021-04-26 18:04:55,073 >> creating metadata file for /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|file_utils.py:1426] 2021-04-26 18:04:55,291 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphadzegcv\n",
            "Downloading: 100% 508k/508k [00:00<00:00, 1.62MB/s]\n",
            "[INFO|file_utils.py:1430] 2021-04-26 18:04:55,815 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|file_utils.py:1433] 2021-04-26 18:04:55,815 >> creating metadata file for /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|file_utils.py:1426] 2021-04-26 18:04:56,437 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9ogvpfhx\n",
            "Downloading: 100% 387/387 [00:00<00:00, 315kB/s]\n",
            "[INFO|file_utils.py:1430] 2021-04-26 18:04:56,643 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|file_utils.py:1433] 2021-04-26 18:04:56,643 >> creating metadata file for /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|file_utils.py:1426] 2021-04-26 18:04:56,857 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqzyt1wbb\n",
            "Downloading: 100% 620/620 [00:00<00:00, 527kB/s]\n",
            "[INFO|file_utils.py:1430] 2021-04-26 18:04:57,073 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|file_utils.py:1433] 2021-04-26 18:04:57,073 >> creating metadata file for /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-26 18:04:57,074 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-26 18:04:57,074 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-26 18:04:57,074 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-26 18:04:57,074 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-26 18:04:57,074 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-04-26 18:04:57,074 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|file_utils.py:1426] 2021-04-26 18:04:57,420 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeyz2_ova\n",
            "Downloading: 100% 510M/510M [00:07<00:00, 65.8MB/s]\n",
            "[INFO|file_utils.py:1430] 2021-04-26 18:05:05,404 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|file_utils.py:1433] 2021-04-26 18:05:05,404 >> creating metadata file for /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|modeling_utils.py:1052] 2021-04-26 18:05:05,404 >> loading weights file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|modeling_utils.py:1168] 2021-04-26 18:05:10,040 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1177] 2021-04-26 18:05:10,040 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at datificate/gpt2-small-spanish.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 5/5 [00:00<00:00,  9.44ba/s]\n",
            "100% 5/5 [00:00<00:00, 10.59ba/s]\n",
            "100% 5/5 [00:03<00:00,  1.56ba/s]\n",
            "100% 5/5 [00:04<00:00,  1.07ba/s]\n",
            "[INFO|trainer.py:402] 2021-04-26 18:05:25,652 >> Using amp fp16 backend\n",
            "[INFO|trainer.py:1013] 2021-04-26 18:05:25,861 >> ***** Running training *****\n",
            "[INFO|trainer.py:1014] 2021-04-26 18:05:25,861 >>   Num examples = 3252\n",
            "[INFO|trainer.py:1015] 2021-04-26 18:05:25,861 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1016] 2021-04-26 18:05:25,861 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1017] 2021-04-26 18:05:25,861 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:1018] 2021-04-26 18:05:25,861 >>   Gradient Accumulation steps = 12\n",
            "[INFO|trainer.py:1019] 2021-04-26 18:05:25,861 >>   Total optimization steps = 2010\n",
            "{'loss': 0.4069, 'learning_rate': 2.2537313432835822e-05, 'epoch': 7.46}\n",
            " 25% 500/2010 [10:54<32:32,  1.29s/it][INFO|trainer.py:1648] 2021-04-26 18:16:20,810 >> Saving model checkpoint to /output/gpt2-small-spanish-adaptado/checkpoint-500\n",
            "[INFO|configuration_utils.py:329] 2021-04-26 18:16:20,812 >> Configuration saved in /output/gpt2-small-spanish-adaptado/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:831] 2021-04-26 18:16:22,420 >> Model weights saved in /output/gpt2-small-spanish-adaptado/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1901] 2021-04-26 18:16:22,424 >> tokenizer config file saved in /output/gpt2-small-spanish-adaptado/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-26 18:16:22,425 >> Special tokens file saved in /output/gpt2-small-spanish-adaptado/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.2955, 'learning_rate': 1.5074626865671644e-05, 'epoch': 14.92}\n",
            " 50% 1000/2010 [21:55<21:35,  1.28s/it][INFO|trainer.py:1648] 2021-04-26 18:27:21,768 >> Saving model checkpoint to /output/gpt2-small-spanish-adaptado/checkpoint-1000\n",
            "[INFO|configuration_utils.py:329] 2021-04-26 18:27:21,768 >> Configuration saved in /output/gpt2-small-spanish-adaptado/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:831] 2021-04-26 18:27:23,355 >> Model weights saved in /output/gpt2-small-spanish-adaptado/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1901] 2021-04-26 18:27:23,358 >> tokenizer config file saved in /output/gpt2-small-spanish-adaptado/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-26 18:27:23,360 >> Special tokens file saved in /output/gpt2-small-spanish-adaptado/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.2576, 'learning_rate': 7.6119402985074636e-06, 'epoch': 22.38}\n",
            " 75% 1500/2010 [32:52<10:54,  1.28s/it][INFO|trainer.py:1648] 2021-04-26 18:38:18,068 >> Saving model checkpoint to /output/gpt2-small-spanish-adaptado/checkpoint-1500\n",
            "[INFO|configuration_utils.py:329] 2021-04-26 18:38:18,069 >> Configuration saved in /output/gpt2-small-spanish-adaptado/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:831] 2021-04-26 18:38:19,612 >> Model weights saved in /output/gpt2-small-spanish-adaptado/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1901] 2021-04-26 18:38:19,617 >> tokenizer config file saved in /output/gpt2-small-spanish-adaptado/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-26 18:38:19,618 >> Special tokens file saved in /output/gpt2-small-spanish-adaptado/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.2377, 'learning_rate': 1.4925373134328358e-07, 'epoch': 29.84}\n",
            "100% 2000/2010 [43:46<00:12,  1.27s/it][INFO|trainer.py:1648] 2021-04-26 18:49:12,050 >> Saving model checkpoint to /output/gpt2-small-spanish-adaptado/checkpoint-2000\n",
            "[INFO|configuration_utils.py:329] 2021-04-26 18:49:12,051 >> Configuration saved in /output/gpt2-small-spanish-adaptado/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:831] 2021-04-26 18:49:13,562 >> Model weights saved in /output/gpt2-small-spanish-adaptado/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1901] 2021-04-26 18:49:13,565 >> tokenizer config file saved in /output/gpt2-small-spanish-adaptado/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-26 18:49:13,566 >> Special tokens file saved in /output/gpt2-small-spanish-adaptado/checkpoint-2000/special_tokens_map.json\n",
            "100% 2010/2010 [44:04<00:00,  1.35s/it][INFO|trainer.py:1196] 2021-04-26 18:49:30,455 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2644.5942, 'train_samples_per_second': 0.76, 'epoch': 29.99}\n",
            "100% 2010/2010 [44:04<00:00,  1.32s/it]\n",
            "[INFO|trainer.py:1648] 2021-04-26 18:49:30,622 >> Saving model checkpoint to /output/gpt2-small-spanish-adaptado\n",
            "[INFO|configuration_utils.py:329] 2021-04-26 18:49:30,623 >> Configuration saved in /output/gpt2-small-spanish-adaptado/config.json\n",
            "[INFO|modeling_utils.py:831] 2021-04-26 18:49:31,978 >> Model weights saved in /output/gpt2-small-spanish-adaptado/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1901] 2021-04-26 18:49:31,982 >> tokenizer config file saved in /output/gpt2-small-spanish-adaptado/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-26 18:49:31,982 >> Special tokens file saved in /output/gpt2-small-spanish-adaptado/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:722] 2021-04-26 18:49:32,070 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   epoch                      =      29.99\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   init_mem_cpu_alloc_delta   =     1545MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   init_mem_gpu_alloc_delta   =      487MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   train_mem_cpu_alloc_delta  =     -177MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   train_mem_cpu_peaked_delta =      203MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   train_mem_gpu_alloc_delta  =     1431MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   train_mem_gpu_peaked_delta =     1041MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   train_runtime              = 0:44:04.59\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   train_samples              =       3252\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:32,070 >>   train_samples_per_second   =       0.76\n",
            "04/26/2021 18:49:32 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1865] 2021-04-26 18:49:32,196 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1866] 2021-04-26 18:49:32,196 >>   Num examples = 3252\n",
            "[INFO|trainer.py:1867] 2021-04-26 18:49:32,196 >>   Batch size = 8\n",
            "100% 407/407 [00:21<00:00, 19.11it/s]\n",
            "[INFO|trainer_pt_utils.py:722] 2021-04-26 18:49:53,670 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   epoch                     =      29.99\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_loss                 =     0.2222\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_mem_gpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_mem_gpu_peaked_delta =      696MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_runtime              = 0:00:21.35\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_samples              =       3252\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   eval_samples_per_second   =    152.292\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-26 18:49:53,671 >>   perplexity                =     1.2489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH7XQJSeFMVK"
      },
      "source": [
        "## 4.2 Copiar modelo a google drive\n",
        "\n",
        "La siguiente celda copia el modelo a una carpta dentro de google drive llamada \"models\" si se quiere poner en otra carpeta adaptar la celda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePd-tslR3C4R",
        "outputId": "b140bd78-11c0-4ff4-8a69-7c8640f5fd3a"
      },
      "source": [
        "# Moviendo el modelo al google drive\n",
        "output_dir = '/content/gdrive/My Drive/models/'\n",
        "!mkdir -p '/content/gdrive/My Drive/models/'\n",
        "!cp -r '/output/gpt2-small-spanish-adaptado' '{output_dir}'\n",
        "!ls '{output_dir}/gpt2-small-spanish-adaptado'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_results.json  config.json\t\t   tokenizer_config.json\n",
            "checkpoint-1000   eval_results.json\t   trainer_state.json\n",
            "checkpoint-1500   merges.txt\t\t   training_args.bin\n",
            "checkpoint-2000   pytorch_model.bin\t   train_results.json\n",
            "checkpoint-500\t  special_tokens_map.json  vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7j4IcIXnW23",
        "outputId": "703def9e-dab8-49e7-f3f4-7f4959993f79"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTJSNNdEEMMQ"
      },
      "source": [
        "## 4.3 Más opciones al script\n",
        "\n",
        "La siguiente celda muestra todas las opciones diponibles en el texto, por si se requiere hacer de modificaciones al entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtIcsUUZEMrF",
        "outputId": "74f76aa0-406d-466d-a0a4-3a09a27e6349"
      },
      "source": [
        "!python run_clm.py -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-22 23:27:55.466270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: run_clm.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
            "                  [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]\n",
            "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                  [--no_use_fast_tokenizer]\n",
            "                  [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
            "                  [--model_revision MODEL_REVISION]\n",
            "                  [--use_auth_token [USE_AUTH_TOKEN]]\n",
            "                  [--dataset_name DATASET_NAME]\n",
            "                  [--dataset_config_name DATASET_CONFIG_NAME]\n",
            "                  [--train_file TRAIN_FILE]\n",
            "                  [--validation_file VALIDATION_FILE]\n",
            "                  [--block_size BLOCK_SIZE]\n",
            "                  [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                  [--validation_split_percentage VALIDATION_SPLIT_PERCENTAGE]\n",
            "                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
            "                  [--output_dir OUTPUT_DIR]\n",
            "                  [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                  [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
            "                  [--do_predict [DO_PREDICT]]\n",
            "                  [--evaluation_strategy {no,steps,epoch}]\n",
            "                  [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                  [--learning_rate LEARNING_RATE]\n",
            "                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                  [--max_grad_norm MAX_GRAD_NORM]\n",
            "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                  [--max_steps MAX_STEPS]\n",
            "                  [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                  [--warmup_steps WARMUP_STEPS] [--logging_dir LOGGING_DIR]\n",
            "                  [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                  [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
            "                  [--save_total_limit SAVE_TOTAL_LIMIT] [--no_cuda [NO_CUDA]]\n",
            "                  [--seed SEED] [--fp16 [FP16]]\n",
            "                  [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                  [--fp16_backend {auto,amp,apex}] [--local_rank LOCAL_RANK]\n",
            "                  [--tpu_num_cores TPU_NUM_CORES]\n",
            "                  [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug [DEBUG]]\n",
            "                  [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                  [--eval_steps EVAL_STEPS]\n",
            "                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                  [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                  [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
            "                  [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                  [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                  [--greater_is_better GREATER_IS_BETTER]\n",
            "                  [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                  [--sharded_ddp [SHARDED_DDP]] [--deepspeed DEEPSPEED]\n",
            "                  [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                  [--adafactor [ADAFACTOR]]\n",
            "                  [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                  [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                  [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                  [--no_dataloader_pin_memory]\n",
            "                  [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        The model checkpoint for weights initialization.Don't\n",
            "                        set if you want to train a model from scratch.\n",
            "  --model_type MODEL_TYPE\n",
            "                        If training from scratch, pass a model type from the\n",
            "                        list: camembert, xlm-roberta, roberta, bert, openai-\n",
            "                        gpt, gpt2, transfo-xl, xlnet, xlm, ctrl, reformer,\n",
            "                        bert-generation, xlm-prophetnet, prophetnet, bart,\n",
            "                        mbart, pegasus, marian, blenderbot, blenderbot-small\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from huggingface.co\n",
            "  --no_use_fast_tokenizer\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not.\n",
            "  --use_fast_tokenizer [USE_FAST_TOKENIZER]\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not.\n",
            "  --model_revision MODEL_REVISION\n",
            "                        The specific model version to use (can be a branch\n",
            "                        name, tag name or commit id).\n",
            "  --use_auth_token [USE_AUTH_TOKEN]\n",
            "                        Will use the token generated when running\n",
            "                        `transformers-cli login` (necessary to use this script\n",
            "                        with private models).\n",
            "  --dataset_name DATASET_NAME\n",
            "                        The name of the dataset to use (via the datasets\n",
            "                        library).\n",
            "  --dataset_config_name DATASET_CONFIG_NAME\n",
            "                        The configuration name of the dataset to use (via the\n",
            "                        datasets library).\n",
            "  --train_file TRAIN_FILE\n",
            "                        The input training data file (a text file).\n",
            "  --validation_file VALIDATION_FILE\n",
            "                        An optional input evaluation data file to evaluate the\n",
            "                        perplexity on (a text file).\n",
            "  --block_size BLOCK_SIZE\n",
            "                        Optional input sequence length after tokenization.The\n",
            "                        training dataset will be truncated in block of this\n",
            "                        size for training.Default to the model max input\n",
            "                        length for single sentence inputs (take into account\n",
            "                        special tokens).\n",
            "  --overwrite_cache [OVERWRITE_CACHE]\n",
            "                        Overwrite the cached training and evaluation sets\n",
            "  --validation_split_percentage VALIDATION_SPLIT_PERCENTAGE\n",
            "                        The percentage of the train set used as validation set\n",
            "                        in case there's no validation split\n",
            "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\n",
            "                        The number of processes to use for the preprocessing.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "                        Overwrite the content of the output directory.Use this\n",
            "                        to continue training if output_dir points to a\n",
            "                        checkpoint directory.\n",
            "  --do_train [DO_TRAIN]\n",
            "                        Whether to run training.\n",
            "  --do_eval [DO_EVAL]   Whether to run eval on the dev set.\n",
            "  --do_predict [DO_PREDICT]\n",
            "                        Whether to run predictions on the test set.\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "                        The evaluation strategy to use.\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss.\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for AdamW.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay for AdamW if we apply some.\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for AdamW optimizer\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for AdamW optimizer\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for AdamW optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
            "                        The scheduler type to use.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir.\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "                        Log the first global_step\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints.Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints\n",
            "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available\n",
            "  --seed SEED           Random seed that will be set at the beginning of\n",
            "                        training.\n",
            "  --fp16 [FP16]         Whether to use 16-bit (mixed) precision (through\n",
            "                        NVIDIA Apex) instead of 32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --fp16_backend {auto,amp,apex}\n",
            "                        The backend to be used for mixed precision.\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "                        Deprecated, the use of `--debug` is preferred. TPU:\n",
            "                        Whether to print debug metrics\n",
            "  --debug [DEBUG]       Whether to print debug metrics on TPU\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size.\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps.\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process.\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step.\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging.\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "  --no_remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels.\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training.\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not.\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "                        When resuming training, whether or not to skip the\n",
            "                        first epochs and batches to get to the same training\n",
            "                        data.\n",
            "  --sharded_ddp [SHARDED_DDP]\n",
            "                        Whether or not to use sharded DDP training (in\n",
            "                        distributed training only).\n",
            "  --deepspeed DEEPSPEED\n",
            "                        Enable deepspeed and pass the path to deepspeed json\n",
            "                        config file (e.g. ds_config.json)\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "                        The label smoothing epsilon to apply (zero means no\n",
            "                        label smoothing).\n",
            "  --adafactor [ADAFACTOR]\n",
            "                        Whether or not to replace AdamW by Adafactor.\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "                        Whether or not to group samples of roughly the same\n",
            "                        length together when batching.\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "                        The list of integrations to report the results and\n",
            "                        logs to.\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `find_unused_parameters` passed to\n",
            "                        `DistributedDataParallel`.\n",
            "  --no_dataloader_pin_memory\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "                        Whether or not to pin memory for DataLoader.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkhAsvah3Bv-"
      },
      "source": [
        "## 5. Generando texto\n",
        "\n",
        "La sigueintes celdas generan el texto, la primera recupera el modelo entrenado desde nuestro google drive, y la segunda hace la ejecuión. Uno puede ejecutar la segunda opción tantas veces como vea uno necesario. La opción _max_length_ controla la cantidad de texto generado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62esBZh431vF"
      },
      "source": [
        "from transformers import pipeline\n",
        "model = \"/content/drive/MyDrive/models/gpt2-small-spanish-adaptado\"\n",
        "model_text = pipeline('text-generation',model=model, tokenizer=model,)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8OseFTQ4CAs",
        "outputId": "66d9856d-dd03-4009-8664-d605c415f3bf"
      },
      "source": [
        "print(model_text(\"La muerte era una hermosa flor\",max_length=1000)[0]['generated_text'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "La muerte era una hermosa flor. --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- Tu aliento seductora --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- ¿Por qué no te has olvidado lo que has pasado --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- Va en constante movimiento --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --  --   --   --   --   --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- El alma pura y brillante --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --  --   --   --   --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  -- Cuando de la mañana se encienden --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --   --  --   --   --   --   --   --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2YDLe_iGqGE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}